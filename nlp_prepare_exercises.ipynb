{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d772bbb0-b549-4aaf-bd7b-6b72c8347827",
   "metadata": {},
   "source": [
    "---\n",
    "# NLP Prepare Exercises\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24780773-d083-4f41-9570-9506297f8d75",
   "metadata": {},
   "source": [
    "The end result of this exercise should be a file named `prepare.py` that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ef40a-d9d7-4eed-8ccc-c08a6da4e19f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2b9fb1-90b9-4fe4-b842-db89a39fdfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import acquire as a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163064aa-a4f1-400a-8aeb-4a1a6b676f42",
   "metadata": {},
   "source": [
    "---\n",
    "## 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c2b5b-788b-4e3a-bc24-9192eab9d7b3",
   "metadata": {},
   "source": [
    "Define a function named `basic_clean`. It should take in a string and apply some basic text cleaning to it:\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "900f41fd-cb21-4b62-9dd2-35652a3099a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(some_string):\n",
    "    some_string = some_string.lower()\n",
    "    some_string = unicodedata.normalize('NFKD', some_string).encode('ascii', 'ignore').decode('utf-8')\n",
    "    some_string = re.sub(r\"[^a-z0-9'\\s]\", '', some_string)\n",
    "    return some_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e5b85-8c88-4d51-a000-f0cc7b77c822",
   "metadata": {},
   "source": [
    "---\n",
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af0ce0-a48d-42d8-af02-37bb99084a3a",
   "metadata": {},
   "source": [
    "Define a function named `tokenize`. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28105c84-256f-4f64-b0b1-6a2532db2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_string = 'This is an example sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f7c4ab2-c8cb-40ab-9281-c99fec6205d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(some_string):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    some_string = tokenizer.tokenize(some_string, return_str = True)\n",
    "    return some_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e0f1c-ea52-4c09-83b3-ebefbd5db9bb",
   "metadata": {},
   "source": [
    "---\n",
    "## 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb81960d-48a8-47e8-967d-1ca9003da5b0",
   "metadata": {},
   "source": [
    "Define a function named `stem`. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef62fbc-b073-4d9b-b8d0-eb1e7007799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(some_string):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in some_string.split()]\n",
    "    some_string_stemmed = ' '.join(stems)\n",
    "    return some_string_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b1633-36fb-4f70-b038-b5b8fecfac31",
   "metadata": {},
   "source": [
    "---\n",
    "## 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3e59e-4a03-428e-ad09-d26a7af50ae3",
   "metadata": {},
   "source": [
    "Define a function named `lemmatize`. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8dee2d-6187-4bda-b1ca-bb5e3dd352bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(some_string):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in some_string.split()]\n",
    "    some_string_lemmatized = ' '.join(lemmas)\n",
    "    return some_string_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05eb88-4736-4512-80e6-b38f6b7d9edb",
   "metadata": {},
   "source": [
    "---\n",
    "## 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433b0f1-f8bd-4385-9fa0-e97350448b47",
   "metadata": {},
   "source": [
    "Define a function named `remove_stopwords`. It should accept some text and return the text after removing all the stopwords.\n",
    "- This function should define two optional parameters, `extra_words` and `exclude_words`. These parameters should define any additional stop words to include, and any words that we *don't* want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f52653-2511-488a-9724-e5171e246d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(some_string, extra_words = [], exclude_words = []):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    [stopword_list.append(word) for word in extra_words]\n",
    "    [stopword_list.remove(word) for word in extra_words]\n",
    "    words = some_string.split()\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    some_string_without_stopwords = ' '.join(filtered_words)\n",
    "    return some_string_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab721a02-5cdc-425a-aa13-b5e6128d8207",
   "metadata": {},
   "source": [
    "---\n",
    "## 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9adbc5-19e2-4c18-a3c1-3b91988ef88e",
   "metadata": {},
   "source": [
    "Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe `news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345db3f1-444f-414d-978c-a2dfd1c81b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    'business',\n",
    "    'sports',\n",
    "    'technology',\n",
    "    'entertainment'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b4a52c0-4982-41fe-8c5b-95c0e06e357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = a.get_articles(topics, 'https://inshorts.com/en/read/', 'Codeup Data Science Germain Cohort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "075d3b48-95da-4898-8bf2-956ad116bd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Refer friends &amp; get a chance to win Bitcoin wo...</td>\n",
       "      <td>CoinSwitch Kuber has launched 'CSK Referral Le...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China's new COVID-19 outbreak wipes $4 billion...</td>\n",
       "      <td>China's top hot pot chain has lost $4 billion ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow, 13 years ago: Musk on old video from when...</td>\n",
       "      <td>Tesla CEO and the world's richest person Elon ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shiba Inu jumps 40% to record high after anony...</td>\n",
       "      <td>Meme-based cryptocurrency Shiba Inu (SHIB) jum...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clear Air India dues, purchase tickets in cash...</td>\n",
       "      <td>The Department of Expenditure, which comes und...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Refer friends & get a chance to win Bitcoin wo...   \n",
       "1  China's new COVID-19 outbreak wipes $4 billion...   \n",
       "2  Wow, 13 years ago: Musk on old video from when...   \n",
       "3  Shiba Inu jumps 40% to record high after anony...   \n",
       "4  Clear Air India dues, purchase tickets in cash...   \n",
       "\n",
       "                                             content     topic  \n",
       "0  CoinSwitch Kuber has launched 'CSK Referral Le...  business  \n",
       "1  China's top hot pot chain has lost $4 billion ...  business  \n",
       "2  Tesla CEO and the world's richest person Elon ...  business  \n",
       "3  Meme-based cryptocurrency Shiba Inu (SHIB) jum...  business  \n",
       "4  The Department of Expenditure, which comes und...  business  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debaf4a8-3ae2-4693-a8c3-53bec53119b8",
   "metadata": {},
   "source": [
    "---\n",
    "## 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e8e47-3614-45b8-9493-dba0aab0c3d7",
   "metadata": {},
   "source": [
    "Make another dataframe for the Codeup blog posts. Name the dataframe `codeup_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "065c6ce0-3da3-4938-a531-b20a83f2e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://codeup.com/data-science/why-you-should-become-a-data-scientist/',\n",
    "    'https://codeup.com/data-science/math-in-data-science/',\n",
    "    'https://codeup.com/data-science/transition-into-data-science/',\n",
    "    'https://codeup.com/data-science/data-science-career/',\n",
    "    'https://codeup.com/data-science/what-is-python/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf9f625-3f51-45f9-976b-a404bb636fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = a.get_blog_articles(urls, 'Codeup Data Science Germain Cohort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eefd103-c731-4720-8a6e-a6b0ce5e8ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why You Should Become a Data Scientist</td>\n",
       "      <td>What do you look for in a career? Chances are,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the Math and Stats Principles You Nee...</td>\n",
       "      <td>Coming into our Data Science program, you will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the Transition into Data Science Like?</td>\n",
       "      <td>Alumni Katy Salts and Brandi Reger joined us a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What Data Science Career is For You?</td>\n",
       "      <td>If you’re struggling to see yourself as a data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Python?</td>\n",
       "      <td>If you’ve been digging around our website or r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0             Why You Should Become a Data Scientist   \n",
       "1  What are the Math and Stats Principles You Nee...   \n",
       "2     What is the Transition into Data Science Like?   \n",
       "3               What Data Science Career is For You?   \n",
       "4                                    What is Python?   \n",
       "\n",
       "                                             content  \n",
       "0  What do you look for in a career? Chances are,...  \n",
       "1  Coming into our Data Science program, you will...  \n",
       "2  Alumni Katy Salts and Brandi Reger joined us a...  \n",
       "3  If you’re struggling to see yourself as a data...  \n",
       "4  If you’ve been digging around our website or r...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cdea2d-9aa1-41e4-a7bb-2a19d900b0b4",
   "metadata": {},
   "source": [
    "---\n",
    "## 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f2484-db8b-440e-b0aa-685585ce305c",
   "metadata": {},
   "source": [
    "For each dataframe, produce the following columns:\n",
    "- `title` to hold the title\n",
    "- `original` to hold the original article/post content\n",
    "- `clean` to hold the normalized and tokenized original with the stopwords removed.\n",
    "- `stemmed` to hold the stemmed version of the cleaned data.\n",
    "- `lemmatized` to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "734b03d6-04bf-4b02-8cdd-b389cba75efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_nlp_data(df):\n",
    "    df = df.rename(columns={'content' : 'original'})\n",
    "    df['clean'] = df.original.apply(basic_clean)\n",
    "    df['clean'] = df.clean.apply(tokenize)\n",
    "    df['clean'] = df.clean.apply(remove_stopwords)\n",
    "    df['stemmed'] = df.clean.apply(stem)\n",
    "    df['lemmatized'] = df.clean.apply(lemmatize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d0638b5-5349-48a0-b963-be4e112b0ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>topic</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Refer friends &amp; get a chance to win Bitcoin wo...</td>\n",
       "      <td>CoinSwitch Kuber has launched 'CSK Referral Le...</td>\n",
       "      <td>business</td>\n",
       "      <td>coinswitch kuber launched ' csk referral leagu...</td>\n",
       "      <td>coinswitch kuber launch ' csk referr leagu ' c...</td>\n",
       "      <td>coinswitch kuber launched ' csk referral leagu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China's new COVID-19 outbreak wipes $4 billion...</td>\n",
       "      <td>China's top hot pot chain has lost $4 billion ...</td>\n",
       "      <td>business</td>\n",
       "      <td>china ' top hot pot chain lost 4 billion marke...</td>\n",
       "      <td>china ' top hot pot chain lost 4 billion marke...</td>\n",
       "      <td>china ' top hot pot chain lost 4 billion marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow, 13 years ago: Musk on old video from when...</td>\n",
       "      <td>Tesla CEO and the world's richest person Elon ...</td>\n",
       "      <td>business</td>\n",
       "      <td>tesla ceo world ' richest person elon musk twe...</td>\n",
       "      <td>tesla ceo world ' richest person elon musk twe...</td>\n",
       "      <td>tesla ceo world ' richest person elon musk twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shiba Inu jumps 40% to record high after anony...</td>\n",
       "      <td>Meme-based cryptocurrency Shiba Inu (SHIB) jum...</td>\n",
       "      <td>business</td>\n",
       "      <td>memebased cryptocurrency shiba inu shib jumped...</td>\n",
       "      <td>memebas cryptocurr shiba inu shib jump 40 hit ...</td>\n",
       "      <td>memebased cryptocurrency shiba inu shib jumped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clear Air India dues, purchase tickets in cash...</td>\n",
       "      <td>The Department of Expenditure, which comes und...</td>\n",
       "      <td>business</td>\n",
       "      <td>department expenditure comes finance ministry ...</td>\n",
       "      <td>depart expenditur come financ ministri wednesd...</td>\n",
       "      <td>department expenditure come finance ministry w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Refer friends & get a chance to win Bitcoin wo...   \n",
       "1  China's new COVID-19 outbreak wipes $4 billion...   \n",
       "2  Wow, 13 years ago: Musk on old video from when...   \n",
       "3  Shiba Inu jumps 40% to record high after anony...   \n",
       "4  Clear Air India dues, purchase tickets in cash...   \n",
       "\n",
       "                                            original     topic  \\\n",
       "0  CoinSwitch Kuber has launched 'CSK Referral Le...  business   \n",
       "1  China's top hot pot chain has lost $4 billion ...  business   \n",
       "2  Tesla CEO and the world's richest person Elon ...  business   \n",
       "3  Meme-based cryptocurrency Shiba Inu (SHIB) jum...  business   \n",
       "4  The Department of Expenditure, which comes und...  business   \n",
       "\n",
       "                                               clean  \\\n",
       "0  coinswitch kuber launched ' csk referral leagu...   \n",
       "1  china ' top hot pot chain lost 4 billion marke...   \n",
       "2  tesla ceo world ' richest person elon musk twe...   \n",
       "3  memebased cryptocurrency shiba inu shib jumped...   \n",
       "4  department expenditure comes finance ministry ...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  coinswitch kuber launch ' csk referr leagu ' c...   \n",
       "1  china ' top hot pot chain lost 4 billion marke...   \n",
       "2  tesla ceo world ' richest person elon musk twe...   \n",
       "3  memebas cryptocurr shiba inu shib jump 40 hit ...   \n",
       "4  depart expenditur come financ ministri wednesd...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  coinswitch kuber launched ' csk referral leagu...  \n",
       "1  china ' top hot pot chain lost 4 billion marke...  \n",
       "2  tesla ceo world ' richest person elon musk twe...  \n",
       "3  memebased cryptocurrency shiba inu shib jumped...  \n",
       "4  department expenditure come finance ministry w...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_nlp_data(news_df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6de41e6b-b035-4669-b4c6-6360f92ffad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why You Should Become a Data Scientist</td>\n",
       "      <td>What do you look for in a career? Chances are,...</td>\n",
       "      <td>look career chances youre looking way make use...</td>\n",
       "      <td>look career chanc your look way make use parti...</td>\n",
       "      <td>look career chance youre looking way make use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the Math and Stats Principles You Nee...</td>\n",
       "      <td>Coming into our Data Science program, you will...</td>\n",
       "      <td>coming data science program need know math sta...</td>\n",
       "      <td>come data scienc program need know math stat h...</td>\n",
       "      <td>coming data science program need know math sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the Transition into Data Science Like?</td>\n",
       "      <td>Alumni Katy Salts and Brandi Reger joined us a...</td>\n",
       "      <td>alumni katy salts brandi reger joined us publi...</td>\n",
       "      <td>alumni kati salt brandi reger join us public p...</td>\n",
       "      <td>alumnus katy salt brandi reger joined u public...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What Data Science Career is For You?</td>\n",
       "      <td>If you’re struggling to see yourself as a data...</td>\n",
       "      <td>youre struggling see data science professional...</td>\n",
       "      <td>your struggl see data scienc profession may fi...</td>\n",
       "      <td>youre struggling see data science professional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Python?</td>\n",
       "      <td>If you’ve been digging around our website or r...</td>\n",
       "      <td>youve digging around website researching tech ...</td>\n",
       "      <td>youv dig around websit research tech tool may ...</td>\n",
       "      <td>youve digging around website researching tech ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0             Why You Should Become a Data Scientist   \n",
       "1  What are the Math and Stats Principles You Nee...   \n",
       "2     What is the Transition into Data Science Like?   \n",
       "3               What Data Science Career is For You?   \n",
       "4                                    What is Python?   \n",
       "\n",
       "                                            original  \\\n",
       "0  What do you look for in a career? Chances are,...   \n",
       "1  Coming into our Data Science program, you will...   \n",
       "2  Alumni Katy Salts and Brandi Reger joined us a...   \n",
       "3  If you’re struggling to see yourself as a data...   \n",
       "4  If you’ve been digging around our website or r...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  look career chances youre looking way make use...   \n",
       "1  coming data science program need know math sta...   \n",
       "2  alumni katy salts brandi reger joined us publi...   \n",
       "3  youre struggling see data science professional...   \n",
       "4  youve digging around website researching tech ...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  look career chanc your look way make use parti...   \n",
       "1  come data scienc program need know math stat h...   \n",
       "2  alumni kati salt brandi reger join us public p...   \n",
       "3  your struggl see data scienc profession may fi...   \n",
       "4  youv dig around websit research tech tool may ...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  look career chance youre looking way make use ...  \n",
       "1  coming data science program need know math sta...  \n",
       "2  alumnus katy salt brandi reger joined u public...  \n",
       "3  youre struggling see data science professional...  \n",
       "4  youve digging around website researching tech ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_nlp_data(codeup_df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b65ceb5-8cf2-4e4b-becf-be121cdd8c84",
   "metadata": {},
   "source": [
    "---\n",
    "## 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b344d-c416-44ec-b71e-1bf122784945",
   "metadata": {},
   "source": [
    "Ask yourself:\n",
    "- If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "    - lemmatized\n",
    "- If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "    - stemmed\n",
    "- If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n",
    "    - stemmed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
